{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Residual Block\n",
    "### x + (x->Conv2d->BatchNorm->PReLU->Conv2d->BatchNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    '''\n",
    "    ResidualBlock Class\n",
    "    Values\n",
    "        channels: the number of channels throughout the residual block, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.PReLU(),\n",
    "\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  PixelShuffle\n",
    "\n",
    "Proposed in [Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network](https://arxiv.org/abs/1609.05158) (Shi et al. 2016), PixelShuffle, also called sub-pixel convolution, is another way to upsample an image.\n",
    "\n",
    "PixelShuffle simply reshapes a $r^2C\\ x\\ H\\ x\\ W$ tensor into a $C\\ x\\ rH\\ x\\ rW$ tensor. This effectively is trading channel information (or depth) for spatial information (width & heigth). Instead of convolving with stride $1/r$ as in deconvolution, the authors think about the weights in the kernel as being spaced $1/r$ pixels apart. When sliding this kernel over an input, the weights that fall between pixels aren't activated and don't need need to be calculated. The total number of activation patterns is thus increased by a factor of $r^2$. This operation is illustrated in the figure below.\n",
    "\n",
    "The algorithm is implemented as `torch.nn.PixelShuffle` in PyTorch.\n",
    "> ![Efficient Sub-pixel CNN](https://github.com/https-deeplearning-ai/GANs-Public/blob/master/SRGAN-PixelShuffle.png?raw=true)\n",
    "*Efficient sub-pixel CNN, taken from Figure 1 of [Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network](https://arxiv.org/abs/1609.05158) (Shi et al. 2016). The PixelShuffle operation (also known as sub-pixel convolution) is shown as the last step on the right.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator (SRResNet)\n",
    "\n",
    "The Generator network is just a bunch of convolutional layers, residual blocks, and pixel shuffling layers! Note that pixel shuffle layers happen near the end.\n",
    "\n",
    "> ![SRGAN Generator](https://github.com/https-deeplearning-ai/GANs-Public/blob/master/SRGAN-Generator.png?raw=true)\n",
    "*SRGAN Generator, taken from Figure 4 of [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802) (Ledig et al. 2017).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The standard SRGAN input has 3 channels, but the HSC & HST data is in a single band and thus has a single channel. So we need to modify the generator to take a specified number of channels as input (very simply change). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        num_input_channels: Number of channel for input images (HST/HSC are 1)\n",
    "        base_channels: number of channels throughout the generator, a scalar\n",
    "        n_ps_blocks: number of PixelShuffle blocks, a scalar\n",
    "        n_res_blocks: number of residual blocks, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self,num_input_channels=1, base_channels=64, n_ps_blocks=2, n_res_blocks=16):\n",
    "        super().__init__()\n",
    "        # Input layer - take a N channels image and projects it into base channels\n",
    "        self.in_layer = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, base_channels, kernel_size=9, padding=4),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "\n",
    "        # B Residual blocks as shown in the above architecture\n",
    "        # We defined ResidualBlock Above\n",
    "        res_blocks = []\n",
    "        for _ in range(n_res_blocks):\n",
    "            res_blocks += [ResidualBlock(base_channels)]\n",
    "\n",
    "        res_blocks += [\n",
    "            nn.Conv2d(base_channels, base_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_channels),\n",
    "        ]\n",
    "        self.res_blocks = nn.Sequential(*res_blocks)\n",
    "\n",
    "        # PixelShuffle blocks\n",
    "        ps_blocks = []\n",
    "        for i in range(n_ps_blocks):\n",
    "            if i == 0:\n",
    "                pix_shuffle = 3\n",
    "                ps_blocks += [\n",
    "                nn.Conv2d(base_channels, 9 * base_channels, kernel_size=3, padding=1),\n",
    "                nn.PixelShuffle(pix_shuffle),\n",
    "                nn.PReLU(),]\n",
    "            else:\n",
    "                pix_shuffle = 2\n",
    "                ps_blocks += [\n",
    "                nn.Conv2d(base_channels, 4 * base_channels, kernel_size=3, padding=1),\n",
    "                nn.PixelShuffle(pix_shuffle),\n",
    "                nn.PReLU(),\n",
    "            ]\n",
    "            \n",
    "        self.ps_blocks = nn.Sequential(*ps_blocks)\n",
    "\n",
    "        # Output layer\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, 1, kernel_size=9, padding=4),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_res = self.in_layer(x)\n",
    "        x = x_res + self.res_blocks(x_res)\n",
    "        x = self.ps_blocks(x)\n",
    "        x = self.out_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Discriminator Block is  a classifier between SR & Real images. Again as opposed to a normal RGB image we need make the input channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SRGAN Generator](https://github.com/https-deeplearning-ai/GANs-Public/blob/master/SRGAN-Discriminator.png?raw=true)\n",
    "*SRGAN Discriminator, taken from Figure 4 of [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802) (Ledig et al. 2017).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class\n",
    "    Values:\n",
    "        num_input_channels: Number of channel for input images (HST/HSC are 1)\n",
    "\n",
    "        base_channels: number of channels in first convolutional layer, a scalar\n",
    "        n_blocks: number of convolutional blocks, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self,num_input_channels=1, base_channels=64, n_blocks=3):\n",
    "        super().__init__()\n",
    "        self.blocks = [\n",
    "            nn.Conv2d(num_input_channels, base_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(base_channels, base_channels, kernel_size=3, padding=1, stride=2),\n",
    "            nn.BatchNorm2d(base_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        ]\n",
    "\n",
    "        cur_channels = base_channels\n",
    "        for i in range(n_blocks):\n",
    "            self.blocks += [\n",
    "                nn.Conv2d(cur_channels, 2 * cur_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(2 * cur_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "                nn.Conv2d(2 * cur_channels, 2 * cur_channels, kernel_size=3, padding=1, stride=2),\n",
    "                nn.BatchNorm2d(2 * cur_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            ]\n",
    "            cur_channels *= 2\n",
    "\n",
    "        self.blocks += [\n",
    "            # You can replicate nn.Linear with pointwise nn.Conv2d i.e. kernel size=1\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(cur_channels, 2 * cur_channels, kernel_size=1, padding=0),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(2 * cur_channels, 1, kernel_size=1, padding=0),\n",
    "\n",
    "            # Apply sigmoid if necessary in loss function for stability\n",
    "            nn.Flatten(),\n",
    "        ]\n",
    "\n",
    "        self.layers = nn.Sequential(*self.blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "The authors formulate the perceptual loss as a weighted sum of content loss (based on the VGG19 network) and adversarial loss.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L} &= \\mathcal{L}_{VGG} + 10^{-3}\\mathcal{L}_{ADV}\n",
    "\\end{align*}\n",
    "\n",
    "**Content Loss**\n",
    "\n",
    "Previous approaches have used MSE loss for content loss, but this objective function tends to produce blurry images. To address this, they add an extra MSE loss term on VGG19 feature maps. So for feature map $\\phi_{5,4}$ (the feature map after the 4th convolution before the 5th max-pooling layer) from the VGG19 network,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}_{VGG} &= \\left|\\left|\\phi_{5,4}(I^{\\text{HR}}) - \\phi_{5,4}(G(I^{\\text{LR}}))\\right|\\right|_2^2\n",
    "\\end{align*}\n",
    "\n",
    "where $I^{\\text{HR}}$ is the original high-resolution image and $I^{\\text{LR}}$ is the corresponding low-resolution image.\n",
    "\n",
    "**Adversarial Loss**\n",
    "\n",
    " formulated as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}_{ADV} &= \\sum_{n=1}^N -\\log D(G(I^{\\text{LR}}))\n",
    "\\end{align*}\n",
    "\n",
    "Note that $-\\log D(G(\\cdot))$ is used instead of $\\log [1 - D(G(\\cdot))]$ for better gradient behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VGG (and download if needed)\n",
    "VGG expects an RGB image. Our I-band HST/HSC images are grayscale. Since the content loss is computed by the L2 distance between the VGG19 feature maps of the SR & HR images, we need to figure out a way send they grayscale images through VGG19. The most obvious way is to just to map the grayscale images to RGB by repeating the images across the depth dimension. So our (100,100,1) & (560,560,1) images -> (100,100,3) & (560,560,3) where each channel has the same information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg19\n",
    "vgg = vgg19(pretrained=True)\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    '''\n",
    "    Loss Class\n",
    "    Implements composite content+adversarial loss for SRGAN\n",
    "    Values:\n",
    "        device: 'cuda' or 'cpu' hardware to put VGG network on, a string\n",
    "    '''\n",
    "\n",
    "    def __init__(self, device='cuda'):\n",
    "        super().__init__()\n",
    "\n",
    "        vgg = vgg19(pretrained=True).to(device)\n",
    "        self.vgg = nn.Sequential(*list(vgg.features)[:-1]).eval()\n",
    "        for p in self.vgg.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    @staticmethod\n",
    "    def img_loss(x_real, x_fake):\n",
    "        return F.mse_loss(x_real, x_fake)\n",
    "\n",
    "    def adv_loss(self, x, is_real):\n",
    "        # If fake we want \"convince\" that it is real\n",
    "        target = torch.zeros_like(x) if is_real else torch.ones_like(x)\n",
    "        return F.binary_cross_entropy_with_logits(x, target)\n",
    "\n",
    "    def vgg_loss(self, x_real, x_fake):\n",
    "        #Copy across channel diension because VGG expects 3 channels\n",
    "        x_real = torch.repeat_interleave(x_real, 3, dim=1)\n",
    "        x_fake = torch.repeat_interleave(x_fake, 3, dim=1)\n",
    "        return F.mse_loss(self.vgg(x_real), self.vgg(x_fake))\n",
    "\n",
    "    def forward(self, generator, discriminator, hr_real, lr_real):\n",
    "        ''' Performs forward pass and returns total losses for G and D '''\n",
    "        hr_fake = generator(lr_real)\n",
    "        fake_preds_for_g = discriminator(hr_fake)\n",
    "        fake_preds_for_d = discriminator(hr_fake.detach())\n",
    "        real_preds_for_d = discriminator(hr_real.detach())\n",
    "\n",
    "        g_loss = (\n",
    "            0.001 * self.adv_loss(fake_preds_for_g, False) + \\\n",
    "            0.006 * self.vgg_loss(hr_real, hr_fake) + \\\n",
    "            self.img_loss(hr_real, hr_fake)\n",
    "        )\n",
    "        d_loss = 0.5 * (\n",
    "            self.adv_loss(real_preds_for_d, True) + \\\n",
    "            self.adv_loss(fake_preds_for_d, False)\n",
    "        )\n",
    "\n",
    "        return g_loss, d_loss, hr_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from astropy.io import fits\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class SR_HST_HSC_Dataset(Dataset):\n",
    "    '''\n",
    "    Dataset Class\n",
    "    Values:\n",
    "        hr_size: spatial size of high-resolution image, a list/tuple\n",
    "        lr_size: spatial size of low-resolution image, a list/tuple\n",
    "        *args/**kwargs: all other arguments for subclassed torchvision dataset\n",
    "    '''\n",
    "\n",
    "    def __init__(self, hst_path: str, hsc_path:str, hr_size: list, lr_size: list ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if hr_size is not None and lr_size is not None:\n",
    "            assert hr_size[0] == 6 * lr_size[0]\n",
    "            assert hr_size[1] == 6 * lr_size[1]\n",
    "        \n",
    "#         # High-res images are cropped and scaled to [-1, 1]\n",
    "#         self.hr_transforms = transforms.Compose([\n",
    "#             transforms.RandomCrop(hr_size),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.Lambda(lambda img: np.array(img)),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "#         ])\n",
    "\n",
    "        \n",
    "        self.hst_path = hst_path\n",
    "        self.hsc_path = hsc_path \n",
    "        self.filenames = os.listdir(hst_path)\n",
    "\n",
    "\n",
    "        self.to_pil = transforms.ToPILImage()\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        \n",
    "    def _load_fits(self, file_path: str) -> np.ndarray:\n",
    "        cutout = fits.open(file_path)\n",
    "        array = cutout[0].data\n",
    "        array = array.astype(np.float32) # Big->little endian\n",
    "        \n",
    "        return array\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "\n",
    "\n",
    "        hst_image = os.path.join(self.hst_path,self.filenames[idx])\n",
    "        hsc_image = os.path.join(self.hsc_path,self.filenames[idx])\n",
    "        \n",
    "        hst_array = self._load_fits(hst_image)\n",
    "        hsc_array = self._load_fits(hsc_image)\n",
    "        \n",
    "        hst_tensor = torch.from_numpy(hst_array)\n",
    "        hsc_tensor = torch.from_numpy(hsc_array)\n",
    "        \n",
    "        return hst_tensor,hsc_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        hrs, lrs = [], []\n",
    "\n",
    "        for hr, lr in batch:\n",
    "            hrs.append(hr)\n",
    "            lrs.append(lr)\n",
    "\n",
    "        return torch.stack(hrs, dim=0), torch.stack(lrs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parse torch version for autocast\n",
    "# ######################################################\n",
    "version = torch.__version__\n",
    "version = tuple(int(n) for n in version.split('.')[:-1])\n",
    "has_autocast = version >= (1, 6)\n",
    "# ######################################################\n",
    "\n",
    "def show_tensor_images(image_tensor):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:4], nrow=4)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "\n",
    "def train_srresnet(srresnet, dataloader, device, lr=1e-4, total_steps=1e6, display_step=500):\n",
    "    srresnet = srresnet.to(device).train()\n",
    "    optimizer = torch.optim.Adam(srresnet.parameters(), lr=lr)\n",
    "\n",
    "    cur_step = 0\n",
    "    mean_loss = 0.0\n",
    "    while cur_step < total_steps:\n",
    "        for hr_real, lr_real in tqdm(dataloader, position=0):\n",
    "            # Conv2d expects (n_samples, channels, height, width)\n",
    "            # So add the channel dimension\n",
    "            hr_real = hr_real.unsqueeze(1).to(device)\n",
    "            lr_real = lr_real.unsqueeze(1).to(device)\n",
    "            \n",
    "            # Enable autocast to FP16 tensors (new feature since torch==1.6.0)\n",
    "            # If you're running older versions of torch, comment this out\n",
    "            # and use NVIDIA apex for mixed/half precision training\n",
    "            if has_autocast:\n",
    "                with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
    "                    hr_fake = srresnet(lr_real)\n",
    "                    loss = Loss.img_loss(hr_real, hr_fake)\n",
    "            else:\n",
    "\n",
    "                hr_fake = srresnet(lr_real)\n",
    "                loss = Loss.img_loss(hr_real, hr_fake)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            mean_loss += loss.item() / display_step\n",
    "\n",
    "            if cur_step % display_step == 0 and cur_step > 0:\n",
    "                print('Step {}: SRResNet loss: {:.5f}'.format(cur_step, mean_loss))\n",
    "                show_tensor_images(lr_real * 2 - 1)\n",
    "                show_tensor_images(hr_fake.to(hr_real.dtype))\n",
    "                show_tensor_images(hr_real)\n",
    "                mean_loss = 0.0\n",
    "\n",
    "            cur_step += 1\n",
    "            if cur_step == total_steps:\n",
    "                break\n",
    "\n",
    "def train_srgan(generator, discriminator, dataloader, device, lr=1e-4, total_steps=2e5, display_step=500):\n",
    "    generator = generator.to(device).train()\n",
    "    discriminator = discriminator.to(device).train()\n",
    "    loss_fn = Loss(device=device)\n",
    "\n",
    "    g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "    g_scheduler = torch.optim.lr_scheduler.LambdaLR(g_optimizer, lambda _: 0.1)\n",
    "    d_scheduler = torch.optim.lr_scheduler.LambdaLR(d_optimizer, lambda _: 0.1)\n",
    "\n",
    "    lr_step = total_steps // 2\n",
    "    cur_step = 0\n",
    "\n",
    "    mean_g_loss = 0.0\n",
    "    mean_d_loss = 0.0\n",
    "\n",
    "    while cur_step < total_steps:\n",
    "        for hr_real, lr_real in tqdm(dataloader, position=0):\n",
    "            hr_real = hr_real.to(device)\n",
    "            lr_real = lr_real.to(device)\n",
    "            \n",
    "            hr_real = hr_real.unsqueeze(1).to(device)\n",
    "            lr_real = lr_real.unsqueeze(1).to(device)\n",
    "            \n",
    "            # Enable autocast to FP16 tensors (new feature since torch==1.6.0)\n",
    "            # If you're running older versions of torch, comment this out\n",
    "            # and use NVIDIA apex for mixed/half precision training\n",
    "            if has_autocast:\n",
    "                with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
    "                    g_loss, d_loss, hr_fake = loss_fn(\n",
    "                        generator, discriminator, hr_real, lr_real,\n",
    "                    )\n",
    "            else:\n",
    "                g_loss, d_loss, hr_fake = loss_fn(\n",
    "                    generator, discriminator, hr_real, lr_real,\n",
    "                )\n",
    "\n",
    "            g_optimizer.zero_grad()\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            d_optimizer.zero_grad()\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            mean_g_loss += g_loss.item() / display_step\n",
    "            mean_d_loss += d_loss.item() / display_step\n",
    "\n",
    "            if cur_step == lr_step:\n",
    "                g_scheduler.step()\n",
    "                d_scheduler.step()\n",
    "                print('Decayed learning rate by 10x.')\n",
    "\n",
    "            if cur_step % display_step == 0 and cur_step > 0:\n",
    "                print('Step {}: Generator loss: {:.5f}, Discriminator loss: {:.5f}'.format(cur_step, mean_g_loss, mean_d_loss))\n",
    "                show_tensor_images(lr_real * 2 - 1)\n",
    "                show_tensor_images(hr_fake.to(hr_real.dtype))\n",
    "                show_tensor_images(hr_real)\n",
    "                mean_g_loss = 0.0\n",
    "                mean_d_loss = 0.0\n",
    "\n",
    "            cur_step += 1\n",
    "            if cur_step == total_steps:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/552 [02:17<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "generator = Generator(n_res_blocks=16, n_ps_blocks=2)\n",
    "hst_path = \"/Users/samuelkahn/Desktop/Astro - Deep Learning/Brants Group/hsc_hst_data/src/data/samples/hst/filtered\"\n",
    "\n",
    "hsc_path = \"/Users/samuelkahn/Desktop/Astro - Deep Learning/Brants Group/hsc_hst_data/src/data/samples/hsc/filtered\"\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    SR_HST_HSC_Dataset(hst_path = hst_path , hsc_path = hsc_path, hr_size=[600, 600], lr_size=[100, 100]), \n",
    "    batch_size=16, pin_memory=True, shuffle=True,\n",
    ")\n",
    "train_srresnet(generator, dataloader, device, lr=1e-4, total_steps=1e5, display_step=50)\n",
    "torch.save(generator, 'srresnet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/552 [06:51<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decayed learning rate by 10x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generator = torch.load('srresnet.pt')\n",
    "discriminator = Discriminator(n_blocks=1, base_channels=8)\n",
    "\n",
    "train_srgan(generator, discriminator, dataloader, device, lr=1e-4, total_steps=1, display_step=1000)\n",
    "torch.save(generator, 'srgenerator.pt')\n",
    "torch.save(discriminator, 'srdiscriminator.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.2'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
